# Autonomous DB 문서 벡터화 파이프라인

## 0) 전체 흐름

- **DBMS_SCHEDULER (poller)** 가 30초~5분마다 실행
- **poller**가 `DBMS_CLOUD.LIST_OBJECTS` 로 버킷 목록을 가져옴
- 처리 이력 테이블을 보고 **"새 파일(또는 변경된 파일)"**만 `doc_ingest_jobs`에 INSERT
- **worker** 스케줄러가 PENDING job을 가져와서 처리 (`FOR UPDATE SKIP LOCKED`)

**처리 단계:**

1. Object Storage에서 다운로드 (`DBMS_CLOUD.GET_OBJECT`) → BLOB 로드
2. (PDF) `DBMS_VECTOR_CHAIN.UTL_TO_TEXT(blob)` → CLOB 텍스트
3. `UTL_TO_CHUNKS(text)` → 청크
4. **임베딩**: `DBMS_CLOUD_AI.GENERATE(..., profile_name => 'OCI_COHERE_EMBED', action => 'embedding')` 로 벡터 생성 (동일 credential `GENAI_CRED` 사용 가능)
5. `doc_chunks`에 저장
6. job 상태 DONE/ERROR 업데이트

> **핵심 포인트:** 중복 방지(idempotency)를 위해 **"파일 고유키(object_name + etag(or md5))"** 를 저장해야 합니다.  
> 그래야 같은 파일을 poller가 반복해서 job에 넣지 않습니다.

---

## 1) 준비물 (사전 설정)

### 1-1. Object Storage 접근 Credential

ADB에서 Object Storage를 조회/다운로드하려면 credential이 필요합니다.

```sql
BEGIN
  DBMS_CLOUD.CREATE_CREDENTIAL(
    credential_name => 'OBJ_STORE_CRED2',
    username        => '<oci-username-or-user-ocid>',
    password        => '<auth-token>'
  );
END;
/
```

### 1-2. GenAI(임베딩) 호출용 Credential

`UTL_TO_EMBEDDING`으로 ocigenai를 쓰려면 OCI GenAI용 credential이 필요합니다.

```sql
-- OCI GenAI(ocigenai)용 credential. Object Storage credential과 별도로 생성.
BEGIN
  DBMS_VECTOR_CHAIN.CREATE_CREDENTIAL(
    credential_name => 'OCI_GENAI_CRED',
    params          => JSON('{
      "user_ocid"       : "<user-ocid>",
      "tenancy_ocid"    : "<tenancy-ocid>",
      "compartment_ocid": "<compartment-ocid>",
      "private_key"     : "<private-key-string-without-BEGIN-END-lines>",
      "fingerprint"     : "<key-fingerprint>"
    }')
  );
END;
/
```

### 1-3. 임베딩용 DBMS_CLOUD_AI 프로필 (권장: Select AI와 동일 credential 사용)

Select AI 채팅이 **GENAI_CRED**로 동작한다면, **같은 credential**로 임베딩도 하려면 **DBMS_CLOUD_AI**에 **임베딩 전용 프로필**을 하나 만듭니다.

```sql
BEGIN
  DBMS_CLOUD_AI.CREATE_PROFILE(
    profile_name => 'OCI_COHERE_EMBED',
    attributes   => '{
      "provider": "oci",
      "credential_name": "GENAI_CRED",
      "oci_compartment_id": "ocid1.tenancy.oc1..aaaaaaaa...",
      "oci_apiformat": "COHERE",
      "embedding_model": "cohere.embed-v4.0"
    }'
  );
END;
/
```

- `oci_compartment_id`: 테넌시 OCID 또는 사용 중인 컴파트먼트 OCID.
- 채팅용 프로필(`GENAI`)과 별도로, **임베딩 전용** 프로필 이름(`OCI_COHERE_EMBED`)만 구분해 두면 됩니다.

---

## 2) 테이블 설계 (Queue + Manifest + Output)

### 2-1. manifest 테이블: "이미 본 오브젝트" 기록

이게 있어야 폴링이 안정적으로 동작합니다.

```sql
CREATE TABLE obj_manifest (
  bucket_name    VARCHAR2(255) NOT NULL,
  object_name    VARCHAR2(1024) NOT NULL,
  etag           VARCHAR2(200),
  size_bytes     NUMBER,
  last_modified  TIMESTAMP,
  first_seen_at  TIMESTAMP DEFAULT SYSTIMESTAMP NOT NULL,
  last_seen_at   TIMESTAMP DEFAULT SYSTIMESTAMP NOT NULL,
  processed_flag CHAR(1) DEFAULT 'N' NOT NULL,  -- N/Y
  CONSTRAINT obj_manifest_pk PRIMARY KEY (bucket_name, object_name, etag)
);

CREATE INDEX obj_manifest_i1 ON obj_manifest(processed_flag, last_seen_at);
```

- **etag**는 보통 오브젝트 내용이 바뀌면 바뀌는 값이라 "버전 키"로 쓰기 좋습니다.
- 만약 `LIST_OBJECTS` 결과에 etag가 없거나 쓰기 어렵다면:  
  `(bucket_name, object_name, size_bytes, last_modified)` 조합을 유사 키로 사용 (덜 완벽)

### 2-2. job queue 테이블

```sql
CREATE TABLE doc_ingest_jobs (
  job_id        NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  bucket_name   VARCHAR2(255) NOT NULL,
  object_name   VARCHAR2(1024) NOT NULL,
  object_uri    VARCHAR2(2000),
  content_type  VARCHAR2(255),
  size_bytes    NUMBER,
  etag          VARCHAR2(200),
  status        VARCHAR2(30) DEFAULT 'PENDING' NOT NULL, -- PENDING/RUNNING/DONE/ERROR
  attempts      NUMBER DEFAULT 0 NOT NULL,
  created_at    TIMESTAMP DEFAULT SYSTIMESTAMP NOT NULL,
  started_at    TIMESTAMP,
  finished_at   TIMESTAMP,
  error_msg     CLOB
);

CREATE INDEX doc_ingest_jobs_i1 ON doc_ingest_jobs(status, created_at);
CREATE UNIQUE INDEX doc_ingest_jobs_u1 ON doc_ingest_jobs(bucket_name, object_name, etag);
```

`UNIQUE (bucket_name, object_name, etag)`를 걸어두면 poller가 실수로 중복 INSERT 해도 DB가 막아줍니다.

### 2-3. 청크/벡터 저장 테이블

```sql
CREATE TABLE doc_chunks (
  job_id        NUMBER NOT NULL,
  chunk_id      NUMBER NOT NULL,
  chunk_offset  NUMBER,
  chunk_length  NUMBER,
  chunk_text    CLOB,
  embed_vector  VECTOR,
  meta_json     JSON,
  CONSTRAINT doc_chunks_pk PRIMARY KEY (job_id, chunk_id),
  CONSTRAINT doc_chunks_fk1 FOREIGN KEY (job_id) REFERENCES doc_ingest_jobs(job_id)
);
```

---

## 3) poller 프로시저: 버킷을 조회해 신규 오브젝트를 Job으로 적재

### 3-1. LIST_OBJECTS로 목록 가져오기

`DBMS_CLOUD.LIST_OBJECTS`는 Object Storage 버킷의 오브젝트 목록을 반환합니다.  
(출력 컬럼은 ADB/버전/옵션에 따라 조금씩 다를 수 있어요. 가장 안전한 방식은 "JSON 출력"을 받아 `JSON_TABLE`로 파싱하는 것입니다.)

여기서는 "개념적으로" 아래 형태로 구현합니다.

#### (A) poller 예시 (개념형, 실무에서 가장 많이 씀)

```sql

CREATE OR REPLACE PROCEDURE poll_object_storage_to_jobs IS
  v_namespace    VARCHAR2(200) := '<your-namespace>';
  v_bucket       VARCHAR2(255) := '<your-bucket>';
  v_region       VARCHAR2(50)  := '<region>'; -- ap-osaka-1 등
  v_uri_base  VARCHAR2(2000);
BEGIN
  v_uri_base :=
    'https://objectstorage.' || v_region || '.oraclecloud.com/n/' ||
    v_namespace || '/b/' || v_bucket || '/o/';

  /* 1) obj_manifest upsert */
  MERGE INTO obj_manifest m
  USING (
    SELECT
      v_bucket AS bucket_name,
      o.object_name,
      /* 아래 컬럼들은 환경에 따라 이름이 다를 수 있습니다.
         우선 object_name/bytes(또는 size)/checksum(또는 etag)/last_modified를 확인하세요. */
      o.bytes       AS size_bytes,
      o.checksum    AS etag,
      o.last_modified
    FROM TABLE(
      DBMS_CLOUD.LIST_OBJECTS(
        credential_name => 'OBJ_STORE_CRED2',
        location_uri    => v_uri_base
      )
    ) o
  ) s
  ON (m.bucket_name = s.bucket_name
      AND m.object_name = s.object_name
      AND m.etag = s.etag)
  WHEN MATCHED THEN
    UPDATE SET
      m.last_seen_at   = SYSTIMESTAMP,
      m.size_bytes     = s.size_bytes,
      m.last_modified  = s.last_modified
  WHEN NOT MATCHED THEN
    INSERT (bucket_name, object_name, etag, size_bytes, last_modified, processed_flag, last_seen_at)
    VALUES (s.bucket_name, s.object_name, s.etag, s.size_bytes, s.last_modified, 'N', SYSTIMESTAMP);

  /* 2) 신규(N) -> job queue */
  INSERT /*+ ignore_row_on_dupkey_index(doc_ingest_jobs doc_ingest_jobs_u1) */
    INTO doc_ingest_jobs(bucket_name, object_name, etag, size_bytes, object_uri, status)
  SELECT
    m.bucket_name,
    m.object_name,
    m.etag,
    m.size_bytes,
    v_uri_base || REPLACE(m.object_name, ' ', '%20') AS object_uri,
    'PENDING'
  FROM obj_manifest m
  WHERE m.bucket_name = v_bucket
    AND m.processed_flag = 'N';

  UPDATE obj_manifest
     SET processed_flag = 'Y'
   WHERE bucket_name = v_bucket
     AND processed_flag = 'N';

  COMMIT;
END;
/
```

**현실 체크 (중요)**

- `LIST_OBJECTS` 반환 형태(테이블/JSON)가 환경마다 달라질 수 있습니다.  
  → 실제 ADB에서 `SELECT * FROM TABLE(DBMS_CLOUD.LIST_OBJECTS(...))` 같은 방식이 가능하면 그 방식이 더 간단합니다.
- 위 코드는 "흐름"이 핵심이고, JSON 경로/필드명은 실제 반환값에 맞게 한번만 맞춰주면 계속 안정적으로 돌 수 있습니다.

---

## 4) worker 프로시저: job 처리 (다운로드→텍스트→청크→임베딩→저장)

### 4-1. job 1개 처리

임베딩은 **DBMS_CLOUD_AI.GENERATE** + **OCI_COHERE_EMBED** 프로필을 사용합니다. Select AI와 동일한 **GENAI_CRED** credential이 사용되므로 ORA-20003을 피할 수 있습니다.

```sql
CREATE OR REPLACE PROCEDURE ingest_one_job(p_job_id IN NUMBER) IS
  v_uri         VARCHAR2(2000);
  v_blob        BLOB;
  v_text        CLOB;
  v_error_msg   VARCHAR2(4000);
  v_embed_clob  CLOB;
  v_embed_array CLOB;
  v_embedding   VECTOR;
BEGIN
  -- job claim
  UPDATE doc_ingest_jobs
     SET status     = 'RUNNING',
         started_at = SYSTIMESTAMP,
         attempts   = attempts + 1
   WHERE job_id = p_job_id
     AND status = 'PENDING';

  IF SQL%ROWCOUNT = 0 THEN RETURN; END IF;

  -- Clean up any existing chunks from previous failed attempts
  DELETE FROM doc_chunks WHERE job_id = p_job_id;

  SELECT object_uri INTO v_uri
    FROM doc_ingest_jobs WHERE job_id = p_job_id;

  -- download object into DATA_PUMP_DIR (Object Storage 전용 credential 쓰려면 OBJ_STORE_CRED2 등으로 변경)
  DBMS_CLOUD.GET_OBJECT(
    credential_name => 'GENAI_CRED',
    object_uri      => v_uri,
    directory_name  => 'DATA_PUMP_DIR',
    file_name       => 'ingest_' || p_job_id
  );

  -- load as BLOB
  SELECT TO_BLOB(BFILENAME('DATA_PUMP_DIR', 'ingest_' || p_job_id))
    INTO v_blob FROM dual;

  -- PDF/TXT -> text
  v_text := DBMS_VECTOR_CHAIN.UTL_TO_TEXT(v_blob);

  -- chunks + embeddings insert (DBMS_CLOUD_AI embedding profile 사용)
  FOR rec IN (
    SELECT JSON_VALUE(TO_CLOB(c.column_value), '$.chunk_id' RETURNING NUMBER) AS chunk_id,
           JSON_VALUE(TO_CLOB(c.column_value), '$.chunk_offset' RETURNING NUMBER) AS chunk_offset,
           JSON_VALUE(TO_CLOB(c.column_value), '$.chunk_length' RETURNING NUMBER) AS chunk_length,
           JSON_VALUE(TO_CLOB(c.column_value), '$.chunk_data') AS chunk_data
      FROM TABLE(DBMS_VECTOR_CHAIN.UTL_TO_CHUNKS(v_text)) c
  ) LOOP
    -- Embedding via DBMS_CLOUD_AI (same credential as Select AI)
    v_embed_clob := DBMS_CLOUD_AI.GENERATE(
      prompt       => rec.chunk_data,
      profile_name => 'OCI_COHERE_EMBED',
      action       => 'embedding'
    );

    -- Response format: {"embeddings": [[0.1, 0.2, ...]]}
    SELECT JSON_VALUE(v_embed_clob, '$.embeddings[0]')
      INTO v_embed_array FROM dual;

    v_embedding := TO_VECTOR(v_embed_array);

    INSERT INTO doc_chunks(
      job_id, chunk_id, chunk_offset, chunk_length,
      chunk_text, embed_vector, meta_json
    ) VALUES (
      p_job_id,
      rec.chunk_id,
      rec.chunk_offset,
      rec.chunk_length,
      rec.chunk_data,
      v_embedding,
      JSON_OBJECT('source_uri' VALUE v_uri)
    );
  END LOOP;

  UPDATE doc_ingest_jobs
     SET status = 'DONE',
         finished_at = SYSTIMESTAMP,
         error_msg = NULL
   WHERE job_id = p_job_id;

  COMMIT;

EXCEPTION WHEN OTHERS THEN
  v_error_msg := SQLERRM;
  UPDATE doc_ingest_jobs
     SET status = 'ERROR',
         finished_at = SYSTIMESTAMP,
         error_msg = v_error_msg
   WHERE job_id = p_job_id;
  COMMIT;
  RAISE;
END;
/
```

### 4-2. worker가 "PENDING 1개"를 가져오는 래퍼 (동시성 안전)

```sql
CREATE OR REPLACE PROCEDURE ingest_worker IS
  v_job_id NUMBER;
BEGIN
  -- ORDER BY + FETCH와 FOR UPDATE는 함께 쓸 수 없음 (ORA-02014).
  -- PENDING 1건만 골라서 가져오고, 실제 "점유"는 ingest_one_job 내부 UPDATE로 처리.
  SELECT job_id
    INTO v_job_id
    FROM doc_ingest_jobs
   WHERE status = 'PENDING'
   ORDER BY created_at
   FETCH FIRST 1 ROWS ONLY;

  ingest_one_job(v_job_id);

EXCEPTION
  WHEN NO_DATA_FOUND THEN
    NULL;
END;
/
```

## 5) DBMS_SCHEDULER 설정 (poller + worker)

### 5-1. poller: 1분마다

```sql
BEGIN
  DBMS_SCHEDULER.CREATE_JOB(
    job_name        => 'OBJ_POLL_JOB',
    job_type        => 'STORED_PROCEDURE',
    job_action      => 'POLL_OBJECT_STORAGE_TO_JOBS',
    start_date      => SYSTIMESTAMP,
    repeat_interval => 'FREQ=MINUTELY;INTERVAL=1',
    enabled         => TRUE,
    auto_drop       => FALSE
  );
END;
/
```

### 5-2. worker: 30초마다 (또는 더 자주)

```sql
BEGIN
  DBMS_SCHEDULER.CREATE_JOB(
    job_name        => 'INGEST_WORKER_01',
    job_type        => 'STORED_PROCEDURE',
    job_action      => 'INGEST_WORKER',
    start_date      => SYSTIMESTAMP,
    repeat_interval => 'FREQ=SECONDLY;INTERVAL=30',
    enabled         => TRUE,
    auto_drop       => FALSE
  );

  -- 처리량 필요하면 워커를 여러 개
  DBMS_SCHEDULER.CREATE_JOB(
    job_name        => 'INGEST_WORKER_02',
    job_type        => 'STORED_PROCEDURE',
    job_action      => 'INGEST_WORKER',
    start_date      => SYSTIMESTAMP,
    repeat_interval => 'FREQ=SECONDLY;INTERVAL=30',
    enabled         => TRUE,
    auto_drop       => FALSE
  );
END;
/
```

---
