# Autonomous DB 문서 벡터화 파이프라인

## 0) 전체 흐름

- **DBMS_SCHEDULER (poller)** 가 30초~5분마다 실행
- **poller**가 `DBMS_CLOUD.LIST_OBJECTS` 로 버킷 목록을 가져옴
- 처리 이력 테이블을 보고 **"새 파일(또는 변경된 파일)"**만 `doc_ingest_jobs`에 INSERT
- **worker** 스케줄러가 PENDING job을 가져와서 처리 (`FOR UPDATE SKIP LOCKED`)

**처리 단계:**

1. Object Storage에서 다운로드 (`DBMS_CLOUD.GET_OBJECT`) → BLOB 로드
2. (PDF) `DBMS_VECTOR_CHAIN.UTL_TO_TEXT(blob)` → CLOB 텍스트
3. `UTL_TO_CHUNKS(text)` → 청크
4. `UTL_TO_EMBEDDINGS(chunk, provider=ocigenai, model=cohere.embed-v4.0)` → 벡터
5. `doc_chunks`에 저장
6. job 상태 DONE/ERROR 업데이트

> **핵심 포인트:** 중복 방지(idempotency)를 위해 **"파일 고유키(object_name + etag(or md5))"** 를 저장해야 합니다.  
> 그래야 같은 파일을 poller가 반복해서 job에 넣지 않습니다.

---

## 1) 준비물 (사전 설정)

### 1-1. Object Storage 접근 Credential

ADB에서 Object Storage를 조회/다운로드하려면 credential이 필요합니다.

```sql
BEGIN
  DBMS_CLOUD.CREATE_CREDENTIAL(
    credential_name => 'OBJSTORE_CRED',
    username        => '<oci-username-or-user-ocid>',
    password        => '<auth-token>'
  );
END;
/
```

### 1-2. GenAI 호출용 Credential (예: OCI GenAI endpoint)

아래 예시는 개념상 이름만 보여드립니다. (환경에 따라 방식이 다름)

| 항목 | 값 |
|------|-----|
| credential_name | `OCI_GENAI_CRED` |
| embed endpoint URL | `https://inference.generativeai.<region>.oci.oraclecloud.com/20231130/actions/embedText` |

---

## 2) 테이블 설계 (Queue + Manifest + Output)

### 2-1. manifest 테이블: "이미 본 오브젝트" 기록

이게 있어야 폴링이 안정적으로 동작합니다.

```sql
CREATE TABLE obj_manifest (
  bucket_name    VARCHAR2(255) NOT NULL,
  object_name    VARCHAR2(1024) NOT NULL,
  etag           VARCHAR2(200),
  size_bytes     NUMBER,
  last_modified  TIMESTAMP,
  first_seen_at  TIMESTAMP DEFAULT SYSTIMESTAMP NOT NULL,
  last_seen_at   TIMESTAMP DEFAULT SYSTIMESTAMP NOT NULL,
  processed_flag CHAR(1) DEFAULT 'N' NOT NULL,  -- N/Y
  CONSTRAINT obj_manifest_pk PRIMARY KEY (bucket_name, object_name, etag)
);

CREATE INDEX obj_manifest_i1 ON obj_manifest(processed_flag, last_seen_at);
```

- **etag**는 보통 오브젝트 내용이 바뀌면 바뀌는 값이라 "버전 키"로 쓰기 좋습니다.
- 만약 `LIST_OBJECTS` 결과에 etag가 없거나 쓰기 어렵다면:  
  `(bucket_name, object_name, size_bytes, last_modified)` 조합을 유사 키로 사용 (덜 완벽)

### 2-2. job queue 테이블

```sql
CREATE TABLE doc_ingest_jobs (
  job_id        NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  bucket_name   VARCHAR2(255) NOT NULL,
  object_name   VARCHAR2(1024) NOT NULL,
  object_uri    VARCHAR2(2000),
  content_type  VARCHAR2(255),
  size_bytes    NUMBER,
  etag          VARCHAR2(200),
  status        VARCHAR2(30) DEFAULT 'PENDING' NOT NULL, -- PENDING/RUNNING/DONE/ERROR
  attempts      NUMBER DEFAULT 0 NOT NULL,
  created_at    TIMESTAMP DEFAULT SYSTIMESTAMP NOT NULL,
  started_at    TIMESTAMP,
  finished_at   TIMESTAMP,
  error_msg     CLOB
);

CREATE INDEX doc_ingest_jobs_i1 ON doc_ingest_jobs(status, created_at);
CREATE UNIQUE INDEX doc_ingest_jobs_u1 ON doc_ingest_jobs(bucket_name, object_name, etag);
```

`UNIQUE (bucket_name, object_name, etag)`를 걸어두면 poller가 실수로 중복 INSERT 해도 DB가 막아줍니다.

### 2-3. 청크/벡터 저장 테이블

```sql
CREATE TABLE doc_chunks (
  job_id        NUMBER NOT NULL,
  chunk_id      NUMBER NOT NULL,
  chunk_offset  NUMBER,
  chunk_length  NUMBER,
  chunk_text    CLOB,
  embed_vector  VECTOR,
  meta_json     JSON,
  CONSTRAINT doc_chunks_pk PRIMARY KEY (job_id, chunk_id),
  CONSTRAINT doc_chunks_fk1 FOREIGN KEY (job_id) REFERENCES doc_ingest_jobs(job_id)
);
```

---

## 3) poller 프로시저: 버킷을 조회해 신규 오브젝트를 Job으로 적재

### 3-1. LIST_OBJECTS로 목록 가져오기

`DBMS_CLOUD.LIST_OBJECTS`는 Object Storage 버킷의 오브젝트 목록을 반환합니다.  
(출력 컬럼은 ADB/버전/옵션에 따라 조금씩 다를 수 있어요. 가장 안전한 방식은 "JSON 출력"을 받아 `JSON_TABLE`로 파싱하는 것입니다.)

여기서는 "개념적으로" 아래 형태로 구현합니다.

#### (A) poller 예시 (개념형, 실무에서 가장 많이 씀)

```sql
CREATE OR REPLACE PROCEDURE poll_object_storage_to_jobs IS
  v_objects_json CLOB;
  v_namespace    VARCHAR2(200) := '<your-namespace>';
  v_bucket       VARCHAR2(255) := '<your-bucket>';
  v_region       VARCHAR2(50)  := '<region>'; -- ap-osaka-1 등
BEGIN
  -- 1) 버킷 오브젝트 목록(JSON)을 가져왔다고 가정
  -- 환경에 따라 LIST_OBJECTS 결과가 table 형태일 수도, JSON일 수도 있으니
  -- 실제 반환 타입에 맞게 아래 구간을 조정하세요.
  --
  -- v_objects_json := DBMS_CLOUD.LIST_OBJECTS(
  --   credential_name => 'OBJSTORE_CRED',
  --   location_uri    => 'https://objectstorage.'||v_region||'.oraclecloud.com/n/'||v_namespace||'/b/'||v_bucket||'/o/'
  -- );

  -- 2) JSON_TABLE로 object_name/etag/size/mtime 등을 파싱해서 manifest upsert
  MERGE INTO obj_manifest m
  USING (
    SELECT
      v_bucket AS bucket_name,
      jt.object_name,
      jt.etag,
      jt.size_bytes,
      jt.last_modified
    FROM JSON_TABLE(
      v_objects_json,
      '$.objects[*]'
      COLUMNS (
        object_name   VARCHAR2(1024) PATH '$.name',
        etag          VARCHAR2(200)  PATH '$.etag',
        size_bytes    NUMBER         PATH '$.size',
        last_modified TIMESTAMP      PATH '$.timeModified'
      )
    ) jt
  ) s
  ON (m.bucket_name = s.bucket_name AND m.object_name = s.object_name AND m.etag = s.etag)
  WHEN MATCHED THEN
    UPDATE SET
      m.last_seen_at = SYSTIMESTAMP,
      m.size_bytes = s.size_bytes,
      m.last_modified = s.last_modified
  WHEN NOT MATCHED THEN
    INSERT (bucket_name, object_name, etag, size_bytes, last_modified, processed_flag)
    VALUES (s.bucket_name, s.object_name, s.etag, s.size_bytes, s.last_modified, 'N');

  -- 3) processed_flag='N' 인 것들을 job queue에 넣고 Y로 바꿈
  INSERT /*+ ignore_row_on_dupkey_index(doc_ingest_jobs doc_ingest_jobs_u1) */
    INTO doc_ingest_jobs(bucket_name, object_name, etag, size_bytes, object_uri, status)
  SELECT
    m.bucket_name,
    m.object_name,
    m.etag,
    m.size_bytes,
    'https://objectstorage.'||v_region||'.oraclecloud.com/n/'||v_namespace||'/b/'||v_bucket||'/o/'||
      REPLACE(m.object_name, ' ', '%20') AS object_uri,
    'PENDING'
  FROM obj_manifest m
  WHERE m.bucket_name = v_bucket
    AND m.processed_flag = 'N';

  UPDATE obj_manifest
     SET processed_flag = 'Y'
   WHERE bucket_name = v_bucket
     AND processed_flag = 'N';

  COMMIT;
END;
/
```

**현실 체크 (중요)**

- `LIST_OBJECTS` 반환 형태(테이블/JSON)가 환경마다 달라질 수 있습니다.  
  → 실제 ADB에서 `SELECT * FROM TABLE(DBMS_CLOUD.LIST_OBJECTS(...))` 같은 방식이 가능하면 그 방식이 더 간단합니다.
- 위 코드는 "흐름"이 핵심이고, JSON 경로/필드명은 실제 반환값에 맞게 한번만 맞춰주면 계속 안정적으로 돌 수 있습니다.

---

## 4) worker 프로시저: job 처리 (다운로드→텍스트→청크→임베딩→저장)

### 4-1. job 1개 처리

```sql
CREATE OR REPLACE PROCEDURE ingest_one_job(p_job_id IN NUMBER) IS
  v_uri         VARCHAR2(2000);
  v_blob        BLOB;
  v_text        CLOB;
  v_embed_params CLOB;
BEGIN
  -- job claim
  UPDATE doc_ingest_jobs
     SET status     = 'RUNNING',
         started_at = SYSTIMESTAMP,
         attempts   = attempts + 1
   WHERE job_id = p_job_id
     AND status = 'PENDING';

  IF SQL%ROWCOUNT = 0 THEN
    RETURN;
  END IF;

  SELECT object_uri INTO v_uri
    FROM doc_ingest_jobs
   WHERE job_id = p_job_id;

  -- download object into DATA_PUMP_DIR
  DBMS_CLOUD.GET_OBJECT(
    credential_name => 'OBJSTORE_CRED',
    object_uri      => v_uri,
    directory_name  => 'DATA_PUMP_DIR',
    file_name       => 'ingest_' || p_job_id
  );

  -- load as BLOB
  SELECT TO_BLOB(BFILENAME('DATA_PUMP_DIR', 'ingest_' || p_job_id))
    INTO v_blob
    FROM dual;

  -- PDF/TXT -> text
  v_text := DBMS_VECTOR_CHAIN.UTL_TO_TEXT(v_blob);

  -- embed params (update region, credential, etc.)
  v_embed_params := '{
    "provider": "ocigenai",
    "credential_name": "OCI_GENAI_CRED",
    "url": "https://inference.generativeai.<region>.oci.oraclecloud.com/20231130/actions/embedText",
    "model": "cohere.embed-v4.0",
    "batch_size": 10
  }';

  -- chunks + embeddings insert
  INSERT INTO doc_chunks(job_id, chunk_id, chunk_offset, chunk_length, chunk_text, embed_vector, meta_json)
  SELECT
    p_job_id,
    jt.chunk_id,
    jt.chunk_offset,
    jt.chunk_length,
    jt.chunk_data,
    et.embed_vector,
    JSON_OBJECT('source_uri' VALUE v_uri)
  FROM
    JSON_TABLE(
      DBMS_VECTOR_CHAIN.UTL_TO_CHUNKS(v_text),
      '$[*]' COLUMNS (
        chunk_id     NUMBER PATH '$.chunk_id',
        chunk_offset NUMBER PATH '$.chunk_offset',
        chunk_length NUMBER PATH '$.chunk_length',
        chunk_data   CLOB   PATH '$.chunk_data'
      )
    ) jt
    CROSS JOIN
    JSON_TABLE(
      DBMS_VECTOR_CHAIN.UTL_TO_EMBEDDINGS(jt.chunk_data, JSON(v_embed_params)),
      '$[*]' COLUMNS (
        embed_vector VECTOR PATH '$.embed_vector'
      )
    ) et;

  UPDATE doc_ingest_jobs
     SET status = 'DONE',
         finished_at = SYSTIMESTAMP,
         error_msg = NULL
   WHERE job_id = p_job_id;

  COMMIT;

EXCEPTION
  WHEN OTHERS THEN
    UPDATE doc_ingest_jobs
       SET status = 'ERROR',
           finished_at = SYSTIMESTAMP,
           error_msg = SQLERRM
     WHERE job_id = p_job_id;
    COMMIT;
    RAISE;
END;
/
```

### 4-2. worker가 "PENDING 1개"를 가져오는 래퍼 (동시성 안전)

```sql
CREATE OR REPLACE PROCEDURE ingest_worker IS
  v_job_id NUMBER;
BEGIN
  SELECT job_id
    INTO v_job_id
    FROM doc_ingest_jobs
   WHERE status = 'PENDING'
   ORDER BY created_at
   FETCH FIRST 1 ROWS ONLY
   FOR UPDATE SKIP LOCKED;

  ingest_one_job(v_job_id);

EXCEPTION
  WHEN NO_DATA_FOUND THEN
    NULL;
END;
/
```

---

## 5) DBMS_SCHEDULER 설정 (poller + worker)

### 5-1. poller: 1분마다

```sql
BEGIN
  DBMS_SCHEDULER.CREATE_JOB(
    job_name        => 'OBJ_POLL_JOB',
    job_type        => 'STORED_PROCEDURE',
    job_action      => 'POLL_OBJECT_STORAGE_TO_JOBS',
    start_date      => SYSTIMESTAMP,
    repeat_interval => 'FREQ=MINUTELY;INTERVAL=1',
    enabled         => TRUE,
    auto_drop       => FALSE
  );
END;
/
```

### 5-2. worker: 30초마다 (또는 더 자주)

```sql
BEGIN
  DBMS_SCHEDULER.CREATE_JOB(
    job_name        => 'INGEST_WORKER_01',
    job_type        => 'STORED_PROCEDURE',
    job_action      => 'INGEST_WORKER',
    start_date      => SYSTIMESTAMP,
    repeat_interval => 'FREQ=SECONDLY;INTERVAL=30',
    enabled         => TRUE,
    auto_drop       => FALSE
  );

  -- 처리량 필요하면 워커를 여러 개
  DBMS_SCHEDULER.CREATE_JOB(
    job_name        => 'INGEST_WORKER_02',
    job_type        => 'STORED_PROCEDURE',
    job_action      => 'INGEST_WORKER',
    start_date      => SYSTIMESTAMP,
    repeat_interval => 'FREQ=SECONDLY;INTERVAL=30',
    enabled         => TRUE,
    auto_drop       => FALSE
  );
END;
/
```

---

## 6) 운영에서 꼭 넣는 보강 포인트

### A) 파일 타입 분기 (PDF vs TXT)

- `content_type`을 `LIST_OBJECTS`에서 가져오거나, object extension으로 분기
- TXT는 `UTL_TO_TEXT(blob)` 없이 blob→clob 변환 루틴을 따로 둘 수 있음

### B) 재시도 정책

- `attempts < 3`이면 ERROR → 다시 PENDING으로 되돌리는 별도 job (backoff)
- 너무 잦은 재시도는 GenAI 비용/쿼터를 소모하므로 주의

### C) 대용량 파일

- 너무 큰 PDF는 변환 시간이 길 수 있음 → worker가 1건 처리 시간 제한을 가지도록 설계 (예: 별도 timeout/분할)
- chunk size/overlap 튜닝

### D) 중복/변경 감지

- **etag**가 가장 좋음
- 없다면 `(size_bytes, last_modified)`를 같이 저장해 변경을 추정
- "같은 파일명으로 덮어쓰기"가 많으면 etag 필수에 가까움

---

## 7) 빠른 테스트 순서

1. `OBJSTORE_CRED`, `OCI_GENAI_CRED` 만들기
2. 테이블/프로시저 생성
3. 스케줄러 실행
4. Object Storage에 PDF 1개 업로드
5. 아래로 상태 확인

```sql
SELECT * FROM doc_ingest_jobs ORDER BY created_at DESC FETCH FIRST 10 ROWS ONLY;
SELECT COUNT(*) FROM doc_chunks WHERE job_id = :job_id;
```

---

*EN (English) — Full "DB-only polling" workflow*
