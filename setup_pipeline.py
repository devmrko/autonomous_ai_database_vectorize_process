#!/usr/bin/env python3
"""
Setup script for the Document Vectorization Pipeline.
Creates all tables, procedures, and scheduler jobs from scratch.

Usage:
    python setup_pipeline.py --setup          # Full setup (tables + procedures + jobs)
    python setup_pipeline.py --reset          # Drop and recreate everything
    python setup_pipeline.py --create-tables  # Create tables only
    python setup_pipeline.py --create-procs   # Create procedures only
    python setup_pipeline.py --create-jobs    # Create scheduler jobs only
    python setup_pipeline.py --drop-all       # Drop everything (DANGEROUS!)
    python setup_pipeline.py --status         # Show current status
"""

import os
import sys
import argparse
import logging

# Load .env first
try:
    from dotenv import load_dotenv
    load_dotenv(override=True)
except ImportError:
    print("Warning: python-dotenv not installed. Using system environment variables.")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


# ============================================================================
# Configuration - Update these values for your environment
# ============================================================================
CONFIG = {
    "namespace": "apackrsct01",
    "bucket": "knowledge-base",
    "region": "ap-seoul-1",
    "credential_name": "GENAI_CRED",  # OCI credential name in ADB
    "embed_profile": "OCI_COHERE_EMBED",  # Embedding profile name
    # Chunk settings
    "chunk_by": "words",
    "chunk_max": 500,
    "chunk_overlap": 50,
    "chunk_split": "sentence",
}


def get_uri_base():
    """Get Object Storage URI base."""
    return (
        f"https://{CONFIG['namespace']}.objectstorage.{CONFIG['region']}"
        f".oci.customer-oci.com/n/{CONFIG['namespace']}/b/{CONFIG['bucket']}/o/"
    )


# ============================================================================
# SQL Definitions
# ============================================================================

SQL_CREATE_TABLES = """
-- obj_manifest: tracking seen objects from Object Storage
CREATE TABLE obj_manifest (
  bucket_name    VARCHAR2(255) NOT NULL,
  object_name    VARCHAR2(1024) NOT NULL,
  etag           VARCHAR2(200),
  size_bytes     NUMBER,
  last_modified  TIMESTAMP,
  first_seen_at  TIMESTAMP DEFAULT SYSTIMESTAMP NOT NULL,
  last_seen_at   TIMESTAMP DEFAULT SYSTIMESTAMP NOT NULL,
  processed_flag CHAR(1) DEFAULT 'N' NOT NULL,
  CONSTRAINT obj_manifest_pk PRIMARY KEY (bucket_name, object_name, etag)
)
"""

SQL_CREATE_TABLES_2 = """
-- doc_ingest_jobs: job queue for document processing
CREATE TABLE doc_ingest_jobs (
  job_id        NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  bucket_name   VARCHAR2(255) NOT NULL,
  object_name   VARCHAR2(1024) NOT NULL,
  object_uri    VARCHAR2(2000),
  content_type  VARCHAR2(255),
  size_bytes    NUMBER,
  etag          VARCHAR2(200),
  status        VARCHAR2(30) DEFAULT 'PENDING' NOT NULL,
  attempts      NUMBER DEFAULT 0 NOT NULL,
  created_at    TIMESTAMP DEFAULT SYSTIMESTAMP NOT NULL,
  started_at    TIMESTAMP,
  chunked_at    TIMESTAMP,
  finished_at   TIMESTAMP,
  error_msg     CLOB
)
"""

SQL_CREATE_TABLES_3 = """
-- doc_chunks: chunks and vectors
CREATE TABLE doc_chunks (
  job_id        NUMBER NOT NULL,
  chunk_id      NUMBER NOT NULL,
  chunk_offset  NUMBER,
  chunk_length  NUMBER,
  chunk_text    CLOB,
  embed_vector  VECTOR,
  meta_json     JSON,
  CONSTRAINT doc_chunks_pk PRIMARY KEY (job_id, chunk_id),
  CONSTRAINT doc_chunks_fk1 FOREIGN KEY (job_id) REFERENCES doc_ingest_jobs(job_id)
)
"""

SQL_CREATE_INDEXES = """
CREATE INDEX obj_manifest_i1 ON obj_manifest(processed_flag, last_seen_at)
"""

SQL_CREATE_INDEXES_2 = """
CREATE INDEX doc_ingest_jobs_i1 ON doc_ingest_jobs(status, created_at)
"""

SQL_CREATE_INDEXES_3 = """
CREATE UNIQUE INDEX doc_ingest_jobs_u1 ON doc_ingest_jobs(bucket_name, object_name, etag)
"""


def get_sql_poll_procedure():
    """Generate poll_object_storage_to_jobs procedure."""
    return f"""
CREATE OR REPLACE PROCEDURE poll_object_storage_to_jobs IS
  v_uri_base  VARCHAR2(2000) := '{get_uri_base()}';
  v_bucket    VARCHAR2(255) := '{CONFIG["bucket"]}';
BEGIN
  FOR rec IN (
    SELECT o.object_name, o.bytes AS size_bytes, o.checksum AS etag, o.last_modified
      FROM TABLE(
        DBMS_CLOUD.LIST_OBJECTS(
          credential_name => '{CONFIG["credential_name"]}',
          location_uri    => v_uri_base
        )
      ) o
  ) LOOP
    BEGIN
      INSERT INTO obj_manifest (bucket_name, object_name, etag, size_bytes, last_modified, processed_flag, last_seen_at)
      VALUES (v_bucket, rec.object_name, rec.etag, rec.size_bytes, rec.last_modified, 'N', SYSTIMESTAMP);
    EXCEPTION
      WHEN DUP_VAL_ON_INDEX THEN
        UPDATE obj_manifest
           SET last_seen_at = SYSTIMESTAMP, size_bytes = rec.size_bytes, last_modified = rec.last_modified
         WHERE bucket_name = v_bucket AND object_name = rec.object_name AND etag = rec.etag;
    END;
  END LOOP;
  COMMIT;

  FOR rec IN (
    SELECT bucket_name, object_name, etag, size_bytes
      FROM obj_manifest
     WHERE bucket_name = v_bucket AND processed_flag = 'N'
  ) LOOP
    BEGIN
      INSERT INTO doc_ingest_jobs (bucket_name, object_name, etag, size_bytes, object_uri, status)
      VALUES (rec.bucket_name, rec.object_name, rec.etag, rec.size_bytes,
              v_uri_base || UTL_URL.ESCAPE(rec.object_name, TRUE), 'PENDING');
    EXCEPTION
      WHEN DUP_VAL_ON_INDEX THEN NULL;
    END;
  END LOOP;

  UPDATE obj_manifest SET processed_flag = 'Y'
   WHERE bucket_name = v_bucket AND processed_flag = 'N';
  COMMIT;
END;
"""


def get_sql_stage1_procedure():
    """Generate ingest_stage1_chunk procedure."""
    return f"""
CREATE OR REPLACE PROCEDURE ingest_stage1_chunk(p_job_id IN NUMBER) IS
  v_uri         VARCHAR2(2000);
  v_blob        BLOB;
  v_text        CLOB;
  v_error_msg   VARCHAR2(4000);
  v_chunk_params JSON := JSON('{{"by":"{CONFIG["chunk_by"]}","max":{CONFIG["chunk_max"]},"overlap":{CONFIG["chunk_overlap"]},"split":"{CONFIG["chunk_split"]}","normalize":"all"}}');
BEGIN
  UPDATE doc_ingest_jobs
     SET status = 'CHUNKING', started_at = SYSTIMESTAMP, attempts = attempts + 1
   WHERE job_id = p_job_id AND status IN ('PENDING', 'CHUNK_ERROR');
  IF SQL%ROWCOUNT = 0 THEN RETURN; END IF;

  DELETE FROM doc_chunks WHERE job_id = p_job_id;
  SELECT object_uri INTO v_uri FROM doc_ingest_jobs WHERE job_id = p_job_id;

  DBMS_CLOUD.GET_OBJECT(
    credential_name => '{CONFIG["credential_name"]}',
    object_uri      => v_uri,
    directory_name  => 'DATA_PUMP_DIR',
    file_name       => 'ingest_' || p_job_id
  );

  SELECT TO_BLOB(BFILENAME('DATA_PUMP_DIR', 'ingest_' || p_job_id)) INTO v_blob FROM dual;
  v_text := DBMS_VECTOR_CHAIN.UTL_TO_TEXT(v_blob);

  FOR rec IN (
    SELECT JSON_VALUE(TO_CLOB(c.column_value), '$.chunk_id' RETURNING NUMBER) AS chunk_id,
           JSON_VALUE(TO_CLOB(c.column_value), '$.chunk_offset' RETURNING NUMBER) AS chunk_offset,
           JSON_VALUE(TO_CLOB(c.column_value), '$.chunk_length' RETURNING NUMBER) AS chunk_length,
           JSON_VALUE(TO_CLOB(c.column_value), '$.chunk_data') AS chunk_data
      FROM TABLE(DBMS_VECTOR_CHAIN.UTL_TO_CHUNKS(v_text, v_chunk_params)) c
  ) LOOP
    INSERT INTO doc_chunks(job_id, chunk_id, chunk_offset, chunk_length, chunk_text, embed_vector, meta_json)
    VALUES (p_job_id, rec.chunk_id, rec.chunk_offset, rec.chunk_length, rec.chunk_data, NULL,
            JSON_OBJECT('source_uri' VALUE v_uri));
  END LOOP;

  UPDATE doc_ingest_jobs 
     SET status = 'CHUNKED', chunked_at = SYSTIMESTAMP, error_msg = NULL
   WHERE job_id = p_job_id;
  COMMIT;
EXCEPTION WHEN OTHERS THEN
  v_error_msg := SQLERRM;
  UPDATE doc_ingest_jobs SET status = 'CHUNK_ERROR', error_msg = v_error_msg WHERE job_id = p_job_id;
  COMMIT;
  RAISE;
END;
"""


def get_sql_stage2_procedure():
    """Generate ingest_stage2_embed procedure - EXACT copy from README."""
    return f"""
CREATE OR REPLACE PROCEDURE ingest_stage2_embed(p_job_id IN NUMBER) IS
  v_uri         VARCHAR2(2000);
  v_error_msg   VARCHAR2(4000);
  v_embed_clob  CLOB;
  v_embedding   VECTOR;
BEGIN
  -- Job claim: CHUNKED -> EMBEDDING
  UPDATE doc_ingest_jobs
     SET status   = 'EMBEDDING',
         attempts = attempts + 1
   WHERE job_id = p_job_id
     AND status IN ('CHUNKED', 'EMBED_ERROR');

  IF SQL%ROWCOUNT = 0 THEN RETURN; END IF;

  SELECT object_uri INTO v_uri
    FROM doc_ingest_jobs WHERE job_id = p_job_id;

  -- embed_vectorÍ∞Ä NULLÏù∏ Ï≤≠ÌÅ¨Îßå Ï≤òÎ¶¨
  FOR rec IN (
    SELECT chunk_id, chunk_text
      FROM doc_chunks
     WHERE job_id = p_job_id
       AND embed_vector IS NULL
     ORDER BY chunk_id
  ) LOOP
    v_embed_clob := DBMS_CLOUD_AI.GENERATE(
      prompt       => 'Text to embed: "' || rec.chunk_text || '"',
      profile_name => '{CONFIG["embed_profile"]}',
      action       => 'embedding'
    );

    v_embedding := TO_VECTOR(TRIM(v_embed_clob));

    UPDATE doc_chunks
       SET embed_vector = v_embedding
     WHERE job_id = p_job_id
       AND chunk_id = rec.chunk_id;
  END LOOP;

  -- DONE
  UPDATE doc_ingest_jobs
     SET status      = 'DONE',
         finished_at = SYSTIMESTAMP,
         error_msg   = NULL
   WHERE job_id = p_job_id;

  COMMIT;

EXCEPTION WHEN OTHERS THEN
  v_error_msg := SQLERRM;
  UPDATE doc_ingest_jobs
     SET status    = 'EMBED_ERROR',
         error_msg = v_error_msg
   WHERE job_id = p_job_id;
  COMMIT;
  RAISE;
END;
"""


SQL_CHUNK_WORKER = """
CREATE OR REPLACE PROCEDURE chunk_worker IS
  v_job_id NUMBER;
BEGIN
  SELECT job_id INTO v_job_id FROM doc_ingest_jobs
   WHERE status IN ('PENDING', 'CHUNK_ERROR')
   ORDER BY created_at FETCH FIRST 1 ROWS ONLY;
  ingest_stage1_chunk(v_job_id);
EXCEPTION WHEN NO_DATA_FOUND THEN NULL;
END;
"""

SQL_EMBED_WORKER = """
CREATE OR REPLACE PROCEDURE embed_worker IS
  v_job_id NUMBER;
BEGIN
  SELECT job_id INTO v_job_id FROM doc_ingest_jobs
   WHERE status IN ('CHUNKED', 'EMBED_ERROR')
   ORDER BY chunked_at NULLS LAST, created_at FETCH FIRST 1 ROWS ONLY;
  ingest_stage2_embed(v_job_id);
EXCEPTION WHEN NO_DATA_FOUND THEN NULL;
END;
"""

SQL_INGEST_WORKER = """
CREATE OR REPLACE PROCEDURE ingest_worker IS
  v_job_id NUMBER;
BEGIN
  BEGIN
    SELECT job_id INTO v_job_id FROM doc_ingest_jobs
     WHERE status IN ('PENDING', 'CHUNK_ERROR') ORDER BY created_at FETCH FIRST 1 ROWS ONLY;
    ingest_stage1_chunk(v_job_id);
    RETURN;
  EXCEPTION WHEN NO_DATA_FOUND THEN NULL;
  END;
  BEGIN
    SELECT job_id INTO v_job_id FROM doc_ingest_jobs
     WHERE status IN ('CHUNKED', 'EMBED_ERROR') ORDER BY chunked_at NULLS LAST, created_at FETCH FIRST 1 ROWS ONLY;
    ingest_stage2_embed(v_job_id);
  EXCEPTION WHEN NO_DATA_FOUND THEN NULL;
  END;
END;
"""

SQL_VECTOR_INDEX = """
DECLARE
  v_exists NUMBER;
BEGIN
  SELECT COUNT(*) INTO v_exists FROM user_indexes WHERE index_name = 'DOC_CHUNKS_VEC_IDX';
  IF v_exists = 0 THEN
    EXECUTE IMMEDIATE q'[
      CREATE VECTOR INDEX doc_chunks_vec_idx ON doc_chunks (embed_vector)
      ORGANIZATION INMEMORY NEIGHBOR GRAPH DISTANCE COSINE WITH TARGET ACCURACY 95
    ]';
  END IF;
END;
"""


def get_scheduler_jobs_sql():
    """Generate scheduler job creation SQL."""
    return [
        ("OBJ_POLL_JOB", "POLL_OBJECT_STORAGE_TO_JOBS", "FREQ=MINUTELY;INTERVAL=1"),
        ("CHUNK_WORKER_01", "CHUNK_WORKER", "FREQ=SECONDLY;INTERVAL=30"),
        ("EMBED_WORKER_01", "EMBED_WORKER", "FREQ=SECONDLY;INTERVAL=30"),
    ]


# ============================================================================
# Database Connection
# ============================================================================

def get_connection():
    """Get Oracle database connection with wallet support."""
    import oracledb
    
    user = os.getenv("DB_USER")
    password = os.getenv("DB_PASSWORD")
    dsn = os.getenv("DB_DSN")
    wallet_location = os.getenv("WALLET_LOCATION") or os.getenv("TNS_ADMIN")
    wallet_password = os.getenv("PEM_PASSPHRASE") or os.getenv("WALLET_PASSWORD")
    
    logger.info(f"DB_USER: {user}")
    logger.info(f"DB_DSN: {dsn}")
    logger.info(f"WALLET_LOCATION: {wallet_location}")
    
    if not all([user, password, dsn]):
        raise ValueError("Set DB_USER, DB_PASSWORD, DB_DSN in .env")
    
    if not wallet_location:
        raise ValueError("Set WALLET_LOCATION in .env for ADB connection")
    
    # Check wallet files exist
    wallet_files = ["cwallet.sso", "ewallet.p12", "tnsnames.ora"]
    for f in wallet_files:
        path = os.path.join(wallet_location, f)
        exists = os.path.exists(path)
        logger.info(f"  {f}: {'exists' if exists else 'MISSING'}")
    
    # Set TNS_ADMIN for tnsnames.ora lookup
    os.environ["TNS_ADMIN"] = wallet_location
    
    # Try thick mode first (requires Oracle Instant Client)
    logger.info("Trying thick mode connection...")
    try:
        oracledb.init_oracle_client(config_dir=wallet_location)
        logger.info("Oracle client initialized (thick mode)")
    except oracledb.ProgrammingError as e:
        if "already been initialized" in str(e).lower():
            logger.info("Oracle client already initialized")
        elif "cannot be used in" in str(e).lower() or "thin" in str(e).lower():
            logger.info("Thick mode not available, will use thin mode")
        else:
            logger.warning(f"init_oracle_client: {e}")
    except Exception as e:
        logger.info(f"Thick mode not available: {e}")
    
    logger.info(f"Connecting to {dsn}...")
    
    # Try simple connection first (works with thick mode)
    try:
        conn = oracledb.connect(user=user, password=password, dsn=dsn)
        logger.info("Connected successfully!")
        return conn
    except oracledb.Error as e:
        logger.warning(f"Simple connect failed: {e}")
    
    # Try thin mode with wallet params
    logger.info("Trying thin mode with wallet...")
    try:
        conn = oracledb.connect(
            user=user,
            password=password,
            dsn=dsn,
            config_dir=wallet_location,
            wallet_location=wallet_location,
            wallet_password=wallet_password
        )
        logger.info("Connected successfully (thin mode)!")
        return conn
    except oracledb.Error as e:
        logger.error(f"Thin mode connection failed: {e}")
        raise


def execute_sql(conn, sql, ignore_errors=False):
    """Execute SQL statement."""
    cursor = conn.cursor()
    try:
        cursor.execute(sql)
        conn.commit()
        return True
    except Exception as e:
        if ignore_errors:
            logger.warning(f"Ignored error: {e}")
            return False
        raise
    finally:
        cursor.close()


def execute_plsql(conn, plsql):
    """Execute PL/SQL block."""
    cursor = conn.cursor()
    try:
        cursor.execute(plsql)
        conn.commit()
        return True
    except Exception as e:
        logger.error(f"PL/SQL error: {e}")
        raise
    finally:
        cursor.close()


# ============================================================================
# Setup Functions
# ============================================================================

def create_credentials(conn):
    """Create OCI credentials for DBMS_CLOUD access."""
    logger.info("Creating OCI credentials...")
    
    user_ocid = os.getenv("OCI_USER_OCID")
    tenancy_ocid = os.getenv("OCI_TENANCY_OCID")
    fingerprint = os.getenv("OCI_FINGERPRINT")
    private_key_path = os.getenv("OCI_PRIVATE_KEY_PATH")
    
    if not all([user_ocid, tenancy_ocid, fingerprint, private_key_path]):
        logger.warning("OCI credentials not fully configured in .env")
        logger.warning("Required: OCI_USER_OCID, OCI_TENANCY_OCID, OCI_FINGERPRINT, OCI_PRIVATE_KEY_PATH")
        logger.warning("Skipping credential creation - you can create them manually in SQL Developer")
        return False
    
    # Read private key
    if not os.path.exists(private_key_path):
        logger.error(f"Private key file not found: {private_key_path}")
        return False
    
    with open(private_key_path, 'r') as f:
        private_key = f.read()
    
    # Drop existing credential if exists
    drop_sql = f"""
    BEGIN
        DBMS_CLOUD.DROP_CREDENTIAL(credential_name => '{CONFIG["credential_name"]}');
    EXCEPTION WHEN OTHERS THEN NULL;
    END;
    """
    execute_plsql(conn, drop_sql)
    
    # Create credential
    create_sql = f"""
    BEGIN
        DBMS_CLOUD.CREATE_CREDENTIAL(
            credential_name => '{CONFIG["credential_name"]}',
            user_ocid       => '{user_ocid}',
            tenancy_ocid    => '{tenancy_ocid}',
            private_key     => q'[{private_key}]',
            fingerprint     => '{fingerprint}'
        );
    END;
    """
    try:
        execute_plsql(conn, create_sql)
        logger.info(f"  Created credential: {CONFIG['credential_name']}")
        return True
    except Exception as e:
        logger.error(f"  Failed to create credential: {e}")
        return False


def create_ai_profile(conn):
    """Create AI profile for embeddings."""
    logger.info("Creating AI profile for embeddings...")
    
    region = os.getenv("OCI_REGION", "us-chicago-1")
    
    # Drop existing profile if exists
    drop_sql = f"""
    BEGIN
        DBMS_CLOUD_AI.DROP_PROFILE(profile_name => '{CONFIG["embed_profile"]}');
    EXCEPTION WHEN OTHERS THEN NULL;
    END;
    """
    execute_plsql(conn, drop_sql)
    
    # Create profile for OCI Generative AI embedding
    # Uses 'oci' provider with object_list containing model info
    create_sql = f"""
    BEGIN
        DBMS_CLOUD_AI.CREATE_PROFILE(
            profile_name => '{CONFIG["embed_profile"]}',
            attributes   => '{{
                "provider": "oci",
                "credential_name": "{CONFIG["credential_name"]}",
                "object_list": [
                    {{"owner": "ADMIN", "name": "DOC_CHUNKS"}}
                ],
                "model": "cohere.embed-multilingual-v3.0",
                "oci_apiformat": "COHERE",
                "region": "{region}"
            }}'
        );
    END;
    """
    try:
        execute_plsql(conn, create_sql)
        logger.info(f"  Created AI profile: {CONFIG['embed_profile']}")
        return True
    except Exception as e:
        logger.warning(f"  Profile creation with object_list failed: {e}")
        logger.info("  Trying simpler profile format...")
        
        # Try simpler format
        simple_sql = f"""
        BEGIN
            DBMS_CLOUD_AI.CREATE_PROFILE(
                profile_name => '{CONFIG["embed_profile"]}',
                attributes   => '{{
                    "provider": "oci",
                    "credential_name": "{CONFIG["credential_name"]}",
                    "model": "cohere.embed-multilingual-v3.0"
                }}'
            );
        END;
        """
        try:
            execute_plsql(conn, simple_sql)
            logger.info(f"  Created AI profile (simple): {CONFIG['embed_profile']}")
            return True
        except Exception as e2:
            logger.error(f"  Failed to create AI profile: {e2}")
            return False


def create_tables(conn):
    """Create all tables."""
    logger.info("Creating tables...")
    
    tables = [
        ("obj_manifest", SQL_CREATE_TABLES),
        ("doc_ingest_jobs", SQL_CREATE_TABLES_2),
        ("doc_chunks", SQL_CREATE_TABLES_3),
    ]
    
    for name, sql in tables:
        try:
            execute_sql(conn, sql)
            logger.info(f"  Created table: {name}")
        except Exception as e:
            if "ORA-00955" in str(e):  # Table already exists
                logger.info(f"  Table {name} already exists")
            else:
                raise
    
    # Create indexes
    indexes = [
        ("obj_manifest_i1", SQL_CREATE_INDEXES),
        ("doc_ingest_jobs_i1", SQL_CREATE_INDEXES_2),
        ("doc_ingest_jobs_u1", SQL_CREATE_INDEXES_3),
    ]
    
    for name, sql in indexes:
        try:
            execute_sql(conn, sql)
            logger.info(f"  Created index: {name}")
        except Exception as e:
            if "ORA-00955" in str(e) or "ORA-01408" in str(e):  # Already exists
                logger.info(f"  Index {name} already exists")
            else:
                raise


def create_procedures(conn):
    """Create all stored procedures."""
    logger.info("Creating procedures...")
    
    procedures = [
        ("poll_object_storage_to_jobs", get_sql_poll_procedure()),
        ("ingest_stage1_chunk", get_sql_stage1_procedure()),
        ("ingest_stage2_embed", get_sql_stage2_procedure()),
        ("chunk_worker", SQL_CHUNK_WORKER),
        ("embed_worker", SQL_EMBED_WORKER),
        ("ingest_worker", SQL_INGEST_WORKER),
    ]
    
    for name, sql in procedures:
        try:
            execute_sql(conn, sql)
            logger.info(f"  Created procedure: {name}")
        except Exception as e:
            logger.error(f"  Failed to create {name}: {e}")
            raise


def create_vector_index(conn):
    """Create vector index."""
    logger.info("Creating vector index...")
    try:
        execute_plsql(conn, SQL_VECTOR_INDEX)
        logger.info("  Vector index created or already exists")
    except Exception as e:
        logger.warning(f"  Vector index creation skipped: {e}")


def create_scheduler_jobs(conn):
    """Create DBMS_SCHEDULER jobs."""
    logger.info("Creating scheduler jobs...")
    
    for job_name, proc_name, interval in get_scheduler_jobs_sql():
        sql = f"""
        BEGIN
          BEGIN
            DBMS_SCHEDULER.DROP_JOB('{job_name}', force => TRUE);
          EXCEPTION WHEN OTHERS THEN NULL;
          END;
          
          DBMS_SCHEDULER.CREATE_JOB(
            job_name        => '{job_name}',
            job_type        => 'STORED_PROCEDURE',
            job_action      => '{proc_name}',
            start_date      => SYSTIMESTAMP,
            repeat_interval => '{interval}',
            enabled         => FALSE,
            auto_drop       => FALSE
          );
        END;
        """
        try:
            execute_plsql(conn, sql)
            logger.info(f"  Created job: {job_name} (disabled by default)")
        except Exception as e:
            logger.error(f"  Failed to create job {job_name}: {e}")


def drop_all(conn):
    """Drop all tables, procedures, and jobs."""
    logger.warning("Dropping all objects...")
    
    # Drop scheduler jobs
    jobs = ["OBJ_POLL_JOB", "CHUNK_WORKER_01", "EMBED_WORKER_01", "INGEST_WORKER_01", "INGEST_WORKER_02"]
    for job in jobs:
        execute_sql(conn, f"BEGIN DBMS_SCHEDULER.DROP_JOB('{job}', force => TRUE); EXCEPTION WHEN OTHERS THEN NULL; END;")
    logger.info("  Dropped scheduler jobs")
    
    # Drop procedures
    procs = ["poll_object_storage_to_jobs", "ingest_stage1_chunk", "ingest_stage2_embed", 
             "chunk_worker", "embed_worker", "ingest_worker"]
    for proc in procs:
        execute_sql(conn, f"DROP PROCEDURE {proc}", ignore_errors=True)
    logger.info("  Dropped procedures")
    
    # Drop tables (order matters due to FK)
    execute_sql(conn, "DROP TABLE doc_chunks CASCADE CONSTRAINTS", ignore_errors=True)
    execute_sql(conn, "DROP TABLE doc_ingest_jobs CASCADE CONSTRAINTS", ignore_errors=True)
    execute_sql(conn, "DROP TABLE obj_manifest CASCADE CONSTRAINTS", ignore_errors=True)
    logger.info("  Dropped tables")


def show_status(conn):
    """Show current status."""
    cursor = conn.cursor()
    
    print("\n" + "=" * 60)
    print("PIPELINE STATUS")
    print("=" * 60)
    
    # Credentials
    print("\nüîë CREDENTIALS:")
    try:
        cursor.execute("SELECT credential_name FROM user_credentials")
        creds = [r[0] for r in cursor.fetchall()]
        if CONFIG["credential_name"] in creds:
            print(f"  ‚úÖ {CONFIG['credential_name']}")
        else:
            print(f"  ‚ùå {CONFIG['credential_name']} (not found)")
    except Exception:
        print("  ‚ö†Ô∏è  Unable to check credentials")
    
    # AI Profiles
    print("\nü§ñ AI PROFILES:")
    try:
        cursor.execute("SELECT profile_name FROM user_cloud_ai_profiles")
        profiles = [r[0] for r in cursor.fetchall()]
        if CONFIG["embed_profile"] in profiles:
            print(f"  ‚úÖ {CONFIG['embed_profile']}")
        else:
            print(f"  ‚ùå {CONFIG['embed_profile']} (not found)")
    except Exception:
        print("  ‚ö†Ô∏è  Unable to check AI profiles")
    
    # Tables
    print("\nüìã TABLES:")
    cursor.execute("SELECT table_name FROM user_tables WHERE table_name IN ('OBJ_MANIFEST', 'DOC_INGEST_JOBS', 'DOC_CHUNKS')")
    tables = [r[0] for r in cursor.fetchall()]
    for t in ["OBJ_MANIFEST", "DOC_INGEST_JOBS", "DOC_CHUNKS"]:
        status = "‚úÖ" if t in tables else "‚ùå"
        print(f"  {status} {t}")
    
    # Procedures
    print("\nüì¶ PROCEDURES:")
    cursor.execute("SELECT object_name FROM user_objects WHERE object_type = 'PROCEDURE'")
    procs = [r[0] for r in cursor.fetchall()]
    for p in ["POLL_OBJECT_STORAGE_TO_JOBS", "INGEST_STAGE1_CHUNK", "INGEST_STAGE2_EMBED", 
              "CHUNK_WORKER", "EMBED_WORKER", "INGEST_WORKER"]:
        status = "‚úÖ" if p in procs else "‚ùå"
        print(f"  {status} {p}")
    
    # Scheduler jobs
    print("\n‚è∞ SCHEDULER JOBS:")
    cursor.execute("SELECT job_name, enabled, state FROM user_scheduler_jobs")
    jobs = {r[0]: (r[1], r[2]) for r in cursor.fetchall()}
    for j in ["OBJ_POLL_JOB", "CHUNK_WORKER_01", "EMBED_WORKER_01"]:
        if j in jobs:
            enabled, state = jobs[j]
            icon = "üü¢" if enabled == "TRUE" else "üî¥"
            print(f"  {icon} {j}: {state}")
        else:
            print(f"  ‚ùå {j}: NOT CREATED")
    
    # Job counts
    if "DOC_INGEST_JOBS" in tables:
        print("\nüìä INGEST JOBS:")
        cursor.execute("SELECT status, COUNT(*) FROM doc_ingest_jobs GROUP BY status ORDER BY status")
        for status, count in cursor.fetchall():
            print(f"  {status}: {count}")
    
    cursor.close()
    print("\n" + "=" * 60)


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Setup Document Vectorization Pipeline")
    parser.add_argument("--test-connection", action="store_true", help="Test database connection only")
    parser.add_argument("--setup", action="store_true", help="Full setup (credentials + tables + procedures + jobs)")
    parser.add_argument("--reset", action="store_true", help="Drop and recreate everything")
    parser.add_argument("--create-credentials", action="store_true", help="Create OCI credentials and AI profile")
    parser.add_argument("--create-tables", action="store_true", help="Create tables only")
    parser.add_argument("--create-procs", action="store_true", help="Create procedures only")
    parser.add_argument("--create-jobs", action="store_true", help="Create scheduler jobs only")
    parser.add_argument("--drop-all", action="store_true", help="Drop everything (DANGEROUS!)")
    parser.add_argument("--status", action="store_true", help="Show current status")
    parser.add_argument("--enable-jobs", action="store_true", help="Enable all scheduler jobs")
    
    args = parser.parse_args()
    
    if not any(vars(args).values()):
        parser.print_help()
        return
    
    try:
        if args.test_connection:
            logger.info("Testing database connection...")
            conn = get_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT 'Hello from Oracle!' FROM dual")
            result = cursor.fetchone()
            logger.info(f"Query result: {result[0]}")
            cursor.execute("SELECT SYS_CONTEXT('USERENV', 'CURRENT_USER') FROM dual")
            result = cursor.fetchone()
            logger.info(f"Connected as: {result[0]}")
            cursor.close()
            conn.close()
            logger.info("Connection test successful!")
            return
        
        conn = get_connection()
        logger.info("Connected to database")
        
        if args.drop_all:
            confirm = input("‚ö†Ô∏è  This will DROP ALL tables and data! Type 'YES' to confirm: ")
            if confirm == "YES":
                drop_all(conn)
            else:
                print("Aborted.")
                return
        
        if args.reset:
            confirm = input("‚ö†Ô∏è  This will RESET everything! Type 'YES' to confirm: ")
            if confirm == "YES":
                drop_all(conn)
                create_tables(conn)
                create_procedures(conn)
                create_scheduler_jobs(conn)
                create_vector_index(conn)
            else:
                print("Aborted.")
                return
        
        if args.setup:
            create_credentials(conn)
            create_ai_profile(conn)
            create_tables(conn)
            create_procedures(conn)
            create_scheduler_jobs(conn)
            create_vector_index(conn)
        
        if args.create_credentials:
            create_credentials(conn)
            create_ai_profile(conn)
        
        if args.create_tables:
            create_tables(conn)
        
        if args.create_procs:
            create_procedures(conn)
        
        if args.create_jobs:
            create_scheduler_jobs(conn)
        
        if args.enable_jobs:
            logger.info("Enabling scheduler jobs...")
            for job_name, _, _ in get_scheduler_jobs_sql():
                execute_plsql(conn, f"BEGIN DBMS_SCHEDULER.ENABLE('{job_name}'); END;")
                logger.info(f"  Enabled: {job_name}")
        
        if args.status:
            show_status(conn)
        
        conn.close()
        logger.info("Done!")
        
    except Exception as e:
        logger.error(f"Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
